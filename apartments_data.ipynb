{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0iXULDM0MpJmxYv43hDEx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namesakenberg/BrickByBrickML/blob/main/apartments_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf1M7XQB50I4"
      },
      "outputs": [],
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "url = 'https://www.99acres.com/property-in-gurgaon-ffid-page'\n",
        "IFrame(url, height=500, width=1080)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "# Headers set like below:\n",
        "# User Agent\n",
        "headers = {\n",
        "    'authority': 'www.99acres.com',\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    'cache-control': 'no-cache',\n",
        "    'dnt': '1',\n",
        "    'pragma': 'no-cache',\n",
        "    'referer': 'https://www.99acres.com/property-in-gurgaon-ffid',\n",
        "    'sec-ch-ua': '\"Chromium\";v=\"107\", \"Not;A=Brand\";v=\"8\"',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'sec-ch-ua-platform': '\"macOS\"',\n",
        "    'sec-fetch-dest': 'document',\n",
        "    'sec-fetch-mode': 'navigate',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'sec-fetch-user': '?1',\n",
        "    'upgrade-insecure-requests': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/527.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
        "}"
      ],
      "metadata": {
        "id": "muQcy6PN6MgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract function\n",
        "\n",
        "def extract_data(pageSoup):\n",
        "    global i\n",
        "    d = pd.DataFrame()\n",
        "    for soup in pageSoup.select_one('div[data-label=\"SEARCH\"]').select('section[data-hydration-on-demand=\"true\"]'):\n",
        "\n",
        "        # Extract property name and property sub-name\n",
        "        try:\n",
        "            property_name = soup.find('a', class_='projectTuple__projectName').text.strip()\n",
        "            property_sub_name = soup.find('h2', class_='projectTuple__subHeadingWrap').text.strip()\n",
        "            # print(property_name+'\\n'+property_sub_name)\n",
        "            # Extract link\n",
        "            link = soup.select_one('a', class_='projectTuple__projectName')['href']\n",
        "\n",
        "            page = requests.get(link, headers=headers)\n",
        "            dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
        "            top_f=[]\n",
        "            top_facilities = dpageSoup.find('div',id='top-facilities').find_all('div', class_=\"UniquesFacilities__xidFacilitiesCard\")\n",
        "            for facilities in top_facilities :\n",
        "                top_f.append(facilities.text.strip())\n",
        "\n",
        "            # print(top_f)\n",
        "            # Extract Nearbay Locations with Distances\n",
        "            LocationAdvantages = {}\n",
        "            for l in dpageSoup.select_one('div[data-label=\"LOCATION_HIGHLIGHTS\"]').select('div.locAdvantagesCard__locAdCard'):\n",
        "                t = l.find('div').find_all('div')\n",
        "                loaction = t[0].text.strip()\n",
        "                distance = t[1].text.strip()\n",
        "                LocationAdvantages[loaction] = distance\n",
        "\n",
        "            # Extract nearby locations\n",
        "            nearby_elements = soup.find_all('div', class_=\"SliderTagsAndChips__container\")[0].find_all('li', class_ = 'SliderTagsAndChips__item')\n",
        "            nearby = [element.text.strip() for element in nearby_elements]\n",
        "\n",
        "\n",
        "            #price Range\n",
        "            price_range = soup.find('div', class_=\"pageComponent configurationCards__srpCardStyle\").text\n",
        "\n",
        "            # Extract price details\n",
        "            prices_details = {}\n",
        "            price_elements =  soup.find('div', class_ = 'carousel__CarouselBox').find_all('div', class_=\"configurationCards__cardContainer\")\n",
        "            for element in price_elements:\n",
        "                bedroom_type = element.select_one('span.configurationCards__configBandLabel').text.strip()\n",
        "                building_type = element.select_one('span.configurationCards__configBandHeading').text.strip()\n",
        "                area_type = element.select_one('span.configurationCards__cardAreaTypeStyle').text.strip()\n",
        "                area = element.select_one('span.configurationCards__cardAreaSubHeadingOne').text.strip()\n",
        "                price_range = element.select_one('span.configurationCards__cardPriceHeading').text.strip()\n",
        "\n",
        "                prices_details[bedroom_type] = {\n",
        "                    'building_type': building_type,\n",
        "                    'area_type' : area_type,\n",
        "                    'area': area,\n",
        "                    'price-range': price_range\n",
        "                }\n",
        "            # # Print the extracted data\n",
        "            # print(\"Property Name: \", property_name)\n",
        "            # print(\"Property Sub-name: \", property_sub_name)\n",
        "            # print(\"Nearby Locations: \", nearby)\n",
        "            # print(\"Location Advantages: \", LocationAdvantages)\n",
        "            # print(\"Link: \", link)\n",
        "            # print(\"Price Details: \", prices_details)\n",
        "            # print('Top Facilities: ', top_f)\n",
        "\n",
        "\n",
        "            # Create a dictionary with the given variables\n",
        "            data_dict = {\n",
        "                \"PropertyName\": property_name,\n",
        "                \"PropertySubName\": property_sub_name,\n",
        "                \"NearbyLocations\": nearby,\n",
        "                \"LocationAdvantages\": LocationAdvantages,\n",
        "                \"Link\": link,\n",
        "                \"PriceDetails\": prices_details,\n",
        "                \"TopFacilities\": top_f\n",
        "            }\n",
        "\n",
        "            temp_df = pd.DataFrame.from_records([data_dict])\n",
        "            # print(temp_df)\n",
        "            d = pd.concat([d, temp_df], ignore_index=True)\n",
        "\n",
        "\n",
        "        except:\n",
        "            # print('No Data')\n",
        "            pass\n",
        "        i += 1\n",
        "    return d"
      ],
      "metadata": {
        "id": "FAyKaoTa6ORH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pageNumber=1\n",
        "i=1\n",
        "# Create an empty DataFrame\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Specify the file path for the CSV file\n",
        "file_path = \"apartments_data.csv\"\n",
        "# df.to_csv(file_path, mode='a', index=False)\n",
        "\n",
        "while pageNumber < 50:\n",
        "    URL = f'https://www.99acres.com/property-in-gurgaon-ffid-page-{pageNumber}'\n",
        "    page = requests.get(URL, headers=headers)\n",
        "    pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
        "    try:\n",
        "        data = extract_data(pageSoup)\n",
        "\n",
        "        # Append the dictionary as a row in the DataFrame\n",
        "        if df.empty:\n",
        "            df = pd.concat([data, df], ignore_index=True)\n",
        "            data.to_csv(file_path, index=False)\n",
        "        else:\n",
        "            df = pd.concat([data, df], ignore_index=True)\n",
        "            data.to_csv(file_path, mode='a', index=False, header=False)\n",
        "\n",
        "        print(f\"Data Extracted from {pageNumber}  : {data.shape}. Total Data : {df.shape} \")\n",
        "        pageNumber += 1\n",
        "    except:\n",
        "        print(\"Request Might be decline- waiting for 50 sec to request again.\")\n",
        "        time.sleep(50)\n",
        "# print(soup.prettify())"
      ],
      "metadata": {
        "id": "gddkigDa6PoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "3X853qxT6SR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/apartments_data.csv')"
      ],
      "metadata": {
        "id": "9-uBwNO86UC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates()"
      ],
      "metadata": {
        "id": "S1EG63W06VYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}